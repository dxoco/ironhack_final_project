{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from local_utils import detect_lp\n",
    "from os.path import splitext, basename\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import model_from_json\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.applications.mobilenet_v2 import preprocess_input\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import glob\n",
    "import tensorflow\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model successfully...\n"
     ]
    }
   ],
   "source": [
    "def load_model(path):\n",
    "    try:\n",
    "        path = splitext(path)[0]\n",
    "        with open('%s.json' % path, 'r') as json_file:\n",
    "            model_json = json_file.read()\n",
    "        model = model_from_json(model_json, custom_objects={})\n",
    "        model.load_weights('%s.h5' % path)\n",
    "        print(\"Loading model successfully...\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "wpod_net_path = \"wpod-net.json\"\n",
    "wpod_net = load_model(wpod_net_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path,resize=False):\n",
    "    img = cv2.imread(image_path) # reads the parsing image\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # converts it to RGB\n",
    "    img = img / 255 # normalizes the image data to 0â€“1 range\n",
    "    if resize:\n",
    "# set resize = True to resize all images to same dimension of (width = 224, height = 224)\n",
    "#for visualizing purpos\n",
    "        img = cv2.resize(img, (224,224)) \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 44 images...\n"
     ]
    }
   ],
   "source": [
    "# Create a list of image paths \n",
    "image_paths = glob.glob(\"plate_numbers/*.jpg\")\n",
    "print(\"Found %i images...\"%(len(image_paths)))\n",
    "\n",
    "# Visualize data in subplot \n",
    "fig = plt.figure(figsize=(8,8))\n",
    "cols = 5\n",
    "rows = 4\n",
    "fig_list = []\n",
    "for i in range(cols*rows):\n",
    "    fig_list.append(fig.add_subplot(rows,cols,i+1))\n",
    "    title = splitext(basename(image_paths[i]))[0]\n",
    "    fig_list[-1].set_title(title)\n",
    "    img = preprocess_image(image_paths[i],True)\n",
    "    plt.axis(False)\n",
    "    plt.imshow(img)\n",
    "\n",
    "plt.tight_layout(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](resources/cochecitos.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_plate(image_path, Dmax=3500, Dmin=150):\n",
    "    vehicle = preprocess_image(image_path)\n",
    "    ratio = float(max(vehicle.shape[:2])) / min(vehicle.shape[:2])\n",
    "    side = int(ratio * Dmin)\n",
    "    bound_dim = min(side, Dmax)\n",
    "    _ , LpImg, _, cor = detect_lp(wpod_net, vehicle, bound_dim, lp_threshold=0.5)\n",
    "    return LpImg, cor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detect 1 plate(s) in OIP (27)\n",
      "Coordinate of plate(s) in image: \n",
      " [array([[ 60.39824351, 208.09143599, 203.89332867,  56.20013619],\n",
      "       [ 70.25989705, 101.92575911, 149.44307911, 117.77721705],\n",
      "       [  1.        ,   1.        ,   1.        ,   1.        ]])]\n"
     ]
    }
   ],
   "source": [
    "# Obtain plate image and its coordinates from an image\n",
    "test_image = image_paths[19]\n",
    "LpImg,cor = get_plate(test_image)\n",
    "print(\"Detect %i plate(s) in\"%len(LpImg),splitext(basename(test_image))[0])\n",
    "print(\"Coordinate of plate(s) in image: \\n\", cor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize our result\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.axis(False)\n",
    "plt.imshow(preprocess_image(test_image))\n",
    "plt.subplot(1,2,2)\n",
    "plt.axis(False)\n",
    "plt.imshow(LpImg[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](resources/big_and_roi.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_box(image_path, cor, thickness=3): \n",
    "    pts=[]  \n",
    "    x_coordinates=cor[0][0]\n",
    "    y_coordinates=cor[0][1]\n",
    "    # store the top-left, top-right, bottom-left, bottom-right \n",
    "    # of the plate license respectively\n",
    "    for i in range(4):\n",
    "        pts.append([int(x_coordinates[i]),int(y_coordinates[i])])\n",
    "    \n",
    "    pts = np.array(pts, np.int32)\n",
    "    pts = pts.reshape((-1,1,2))\n",
    "    vehicle_image = preprocess_image(image_path)\n",
    "    \n",
    "    cv2.polylines(vehicle_image,[pts],True,(0,255,0),thickness)\n",
    "    return vehicle_image\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.axis(False)\n",
    "plt.imshow(draw_box(test_image,cor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](resources/ROI_verde.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Viualize all obtained plate images \n",
    "fig = plt.figure(figsize=(12,6))\n",
    "cols = 5\n",
    "rows = 4\n",
    "fig_list = []\n",
    "\n",
    "for i in range(cols*rows):\n",
    "    try:\n",
    "        fig_list.append(fig.add_subplot(rows,cols,i+1))\n",
    "        title = splitext(basename(image_paths[i]))[0]\n",
    "        fig_list[-1].set_title(title)\n",
    "        LpImg,_ = get_plate(image_paths[i])\n",
    "        plt.axis(False)\n",
    "        plt.imshow(LpImg[0])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "plt.tight_layout(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](resources/plaquitas.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (len(LpImg)): #check if there is at least one license image\n",
    "    # Scales, calculates absolute values, and converts the result to 8-bit.\n",
    "    plate_image = cv2.convertScaleAbs(LpImg[0], alpha=(255.0))\n",
    "    \n",
    "    # convert to grayscale and blur the image\n",
    "    gray = cv2.cvtColor(plate_image, cv2.COLOR_BGR2GRAY)\n",
    "    # Blur technique is performed to remove noise\n",
    "    blur = cv2.GaussianBlur(gray,(5,5),0)\n",
    "    \n",
    "    # Applied inversed thresh_binary \n",
    "    binary = cv2.threshold(blur, 100, 255,\n",
    "                         cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
    "    ## Applied dilation to increase the white region of the image\n",
    "    kernel3 = cv2.getStructuringElement(cv2.MORPH_RECT, (2, 2))\n",
    "    thre_mor = cv2.morphologyEx(binary, cv2.MORPH_DILATE, kernel3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detect 7 letters...\n"
     ]
    }
   ],
   "source": [
    "# Create sort_contours() function to grab the contour of each digit from left to right\n",
    "def sort_contours(cnts,reverse = False):\n",
    "    i = 0\n",
    "    boundingBoxes = [cv2.boundingRect(c) for c in cnts]\n",
    "    (cnts, boundingBoxes) = zip(*sorted(zip(cnts, boundingBoxes),\n",
    "                                        key=lambda b: b[1][i], reverse=reverse))\n",
    "    return cnts\n",
    "\n",
    "cont, _  = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# creat a copy version \"test_roi\" of plat_image to draw bounding box\n",
    "test_roi = plate_image.copy()\n",
    "\n",
    "# Initialize a list which will be used to append charater image\n",
    "crop_characters = []\n",
    "\n",
    "# define standard width and height of character\n",
    "digit_w, digit_h = 30, 60\n",
    "\n",
    "for c in sort_contours(cont):\n",
    "    (x, y, w, h) = cv2.boundingRect(c)\n",
    "    ratio = h/w\n",
    "    if 1<=ratio<=3.5: # Only select contour with defined ratio\n",
    "        if h/plate_image.shape[0]>=0.5: # Select contour which has the height larger than 50% of the plate\n",
    "            # Draw bounding box arroung digit number\n",
    "            cv2.rectangle(test_roi, (x, y), (x + w, y + h), (0, 255,0), 2)\n",
    "\n",
    "            # Sperate number and gibe prediction\n",
    "            curr_num = thre_mor[y:y+h,x:x+w]\n",
    "            curr_num = cv2.resize(curr_num, dsize=(digit_w, digit_h))\n",
    "            _, curr_num = cv2.threshold(curr_num, 220, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "            crop_characters.append(curr_num)\n",
    "            \n",
    "print(\"Detect {} letters...\".format(len(crop_characters)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14,4))\n",
    "grid = gridspec.GridSpec(ncols=len(crop_characters),nrows=1,figure=fig)\n",
    "\n",
    "for i in range(len(crop_characters)):\n",
    "    fig.add_subplot(grid[i])\n",
    "    plt.axis(False)\n",
    "    plt.imshow(crop_characters[i],cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](resources/pre_label.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_paths = glob.glob(\"resources/dataset_characters/**/*.jpg\")\n",
    "\n",
    "# cols=4\n",
    "# rows=3\n",
    "# fig = plt.figure(figsize=(10,8))\n",
    "# plt.rcParams.update({\"font.size\":14})\n",
    "# grid = gridspec.GridSpec(ncols=cols,nrows=rows,figure=fig)\n",
    "\n",
    "# # create a random list of images will be displayed\n",
    "# np.random.seed(45)\n",
    "# rand = np.random.randint(0,len(dataset_paths),size=(cols*rows))\n",
    "\n",
    "# # Plot example images\n",
    "# for i in range(cols*rows):\n",
    "#     fig.add_subplot(grid[i])\n",
    "#     image = load_img(dataset_paths[rand[i]])\n",
    "#     label = dataset_paths[rand[i]].split(os.path.sep)[-2]\n",
    "#     plt.title('\"{:s}\"'.format(label))\n",
    "#     plt.axis(False)\n",
    "#     plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](resources/letras_4x3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Find 37623 images with 36 classes\n"
     ]
    }
   ],
   "source": [
    "# Arange input data and corresponding labels\n",
    "X=[]\n",
    "labels=[]\n",
    "\n",
    "for image_path in dataset_paths:\n",
    "    label = image_path.split(os.path.sep)[-2]\n",
    "    image=load_img(image_path,target_size=(80,80))\n",
    "    image=img_to_array(image)\n",
    "\n",
    "    X.append(image)\n",
    "    labels.append(label)\n",
    "\n",
    "X = np.array(X,dtype=\"float16\")\n",
    "labels = np.array(labels)\n",
    "\n",
    "# perform one-hot encoding on the labels\n",
    "lb = LabelEncoder()\n",
    "lb.fit(labels)\n",
    "labels = lb.transform(labels)\n",
    "y = to_categorical(labels)\n",
    "\n",
    "# save label file so we can use in another script\n",
    "np.save('license_character_classes.npy', lb.classes_)\n",
    "\n",
    "# split 10% of data as validation set\n",
    "(trainX, testX, trainY, testY) = train_test_split(X, y, test_size=0.10, stratify=y, random_state=42)\n",
    "\n",
    "\n",
    "# generate data augumentation method\n",
    "image_gen = ImageDataGenerator(rotation_range=10,\n",
    "                              width_shift_range=0.1,\n",
    "                              height_shift_range=0.1,\n",
    "                              shear_range=0.1,\n",
    "                              zoom_range=0.1,\n",
    "                              fill_mode=\"nearest\"\n",
    "                              )\n",
    "\n",
    "print(\"[INFO] Find {:d} images with {:d} classes\".format(len(X),len(set(labels))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf_img_rec_model = tf.keras.models.Sequential([\n",
    "    # Note the input shape is the desired size of the image 150x150 with 3 bytes color\n",
    "    # This is the first convolution\n",
    "    tf.keras.layers.Conv2D(128, (3, 3), input_shape=(80, 80, 3)),\n",
    "    tf.keras.layers.Conv2D(128, (3, 3),  activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Dropout(0.05),\n",
    "    # The second convolution\n",
    "    tf.keras.layers.Conv2D(256, (3, 3),  activation='relu'),\n",
    "    tf.keras.layers.Conv2D(256, (3, 3),  activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # Flatten the results to feed into a DNN\n",
    "    tf.keras.layers.Flatten(),\n",
    "    # 128 neuron hidden layer\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(36, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf_img_rec_model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # #pf_img_rec_model_hist = pf_img_rec_model.fit(trainX, trainY, validation_data=(testX, testY), batch_size=64, epochs=15, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](epochs_crop.JPG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model architectur as json file\n",
    "model_json = pf_img_rec_model.to_json()\n",
    "with open(\"pf_img_rec_model_15e.json\", \"w\") as json_file:  # modelin\n",
    "    json_file.write(model_json)\n",
    "\n",
    "pf_img_rec_model.save(\"pf_img_rec_model_weights_15e.h5\")  # weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Model loaded successfully...\n",
      "[INFO] Labels loaded successfully...\n"
     ]
    }
   ],
   "source": [
    "# Load model architecture, weight and labels\n",
    "json_file = open('pf_img_rec_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(loaded_model_json)\n",
    "model.load_weights(\"pf_img_rec_model_weights.h5\")\n",
    "print(\"[INFO] Model loaded successfully...\")\n",
    "\n",
    "labels = LabelEncoder()\n",
    "labels.classes_ = np.load('license_character_classes.npy')\n",
    "print(\"[INFO] Labels loaded successfully...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processing input images and pedict with model\n",
    "def predict_from_model(image,model,labels):\n",
    "    image = cv2.resize(image,(80,80))\n",
    "    image = np.stack((image,)*3, axis=-1)\n",
    "    prediction = labels.inverse_transform([np.argmax(model.predict(image[np.newaxis,:]))])\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,3))\n",
    "cols = len(crop_characters)\n",
    "grid = gridspec.GridSpec(ncols=cols,nrows=1,figure=fig)\n",
    "\n",
    "final_string = ''\n",
    "for i,character in enumerate(crop_characters):\n",
    "    fig.add_subplot(grid[i])\n",
    "    title = np.array2string(predict_from_model(character,model,labels))\n",
    "    plt.title('{}'.format(title.strip(\"'[]\"),fontsize=20))\n",
    "    final_string+=title.strip(\"'[]\")\n",
    "    plt.axis(False)\n",
    "    plt.imshow(character,cmap='gray')\n",
    "\n",
    "print(\"Achieved result: \", final_string)\n",
    "plt.savefig(final_string+'.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](2361BYR.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
